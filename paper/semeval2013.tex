\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2013}
\usepackage{times}
\usepackage{latexsym}
\setlength\titlebox{6.5cm}    % Expanding the titlebox
\usepackage{url}
\usepackage{float}
\usepackage{graphicx}
\floatstyle{boxed}
\restylefloat{figure}

%% already told Els that this was the title, can't change it now.
\title{HLTDI: CL-WSD Using Markov Random Fields for SemEval-2013 Task 10}

\author{Alex Rudnick, Can Liu and Michael Gasser\\
	    Indiana University, School of Informatics and Computing \\
	    {\tt \{alexr,liucan,gasser\}@indiana.edu}}

\date{}

\begin{document}
\maketitle

\begin{abstract}
We present our entries for the SemEval-2013 cross-language word-sense
disambiguation task \cite{task10}. We submitted three systems based on
classifiers trained on local context features, with some elaborations. Our
three systems, in increasing order of complexity, were: maximum entropy
classifiers trained to predict the desired target-language phrase using only
monolingual features (we called this system \emph{L1}); similar classifiers,
but with the desired target-language phrase for the other four languages as
features (\emph{L2}); and lastly, networks of five classifiers, over which we
do loopy belief propagation to solve the classification tasks jointly
(\emph{MRF}).
\end{abstract}

\section{Introduction}
In the cross-language word-sense disambiguation (CL-WSD) task, given an
instance of an ambiguous word used in a context, we want to predict the
appropriate translation into some target language. This setting for WSD has an
immediate application in machine translation, since many words have many
possible translations. Framing the resolution of lexical ambiguities as an
explicit classification task has been shown to be improve machine translation
even in the case of phrase-based SMT systems \cite{carpuatpsd}, which can
mitigate lexical ambiguities through the use of a language model and
phrase-tables with multi-word phrases.

XXX: work in Brown 1991 reference too: 
\cite{Brown91word-sensedisambiguation}

In the Semeval-2013 CL-WSD task \cite{task10}, entrants are asked to build a
system that can provide translations for twenty ambiguous English nouns, given
appropriate contexts. The five target languages in the shared task are Spanish,
Dutch, German, Italian and French. There were two settings for the evaluation,
\emph{best} and \emph{oof}. In either case, systems may present multiple
possible answers for a given translation, although in the \emph{best} setting,
the first answer is given more weight in the evaluation, and this setting
encourages only returning the top answer. In the \emph{oof} setting, systems
are asked to return the top-five most likely translations. For a complete
explanation of the task and its evaluation, please see the shared task
description \cite{task10}.

%% consider: maybe move this to related work?
Following the work of Lefever and Hoste
\shortcite{lefever-hoste-decock:2011:ACL-HLT2011}, we wanted to develop systems
that make use of multiple bitext corpora for the CL-WSD task.  ParaSense, the
system of Lefever and Hoste, takes into account evidence from all of the
available parallel corpora. Let $S$ be the set of five target languages and $t$
be the particular target language of interest at the moment; ParaSense creates
bag-of-words features from the translations of the target sentence into the
languages $S - \lbrace{t \rbrace}$. Given corpora that are parallel over many
languages, this is straightforward to do at training time, however at testing
time it requires the use of a complete MT system into the four other languages,
which is computationally prohibitive. Thus in our work, we have developed
systems that make use of many parallel corpora but require neither a locally
running MT system nor access to an online translation API.

We presented three systems in this shared task, which were variations on the
theme of a maximum entropy classifier for each ambiguous noun, trained on local
context features similar to those used in previous work and familiar from the
WSD literature. The first system, \emph{L1} (``layer one"), uses maximum
entropy classifiers trained on local context features. The second system,
\emph{L2} (``layer two"), is the same as the \emph{L1} system, with the
addition of the correct translations into the other target languages as
features, which at testing time are predicted with \emph{L1} classifiers. The
third system, \emph{MRF} (``Markov random field") uses a network of interacting
classifiers to solve the classification problem for all five target languages
jointly. Our three systems made use of the same training data, which we
extracted from the Europarl Intersection corpus provided by the shared task
organizers.

At the time of the evaluation, our simplest system came in first place for the
out-of-five evaluation for three languages (Spanish, German, and Italian).
However, after the evaluation deadline, we fixed a simple (slightly
embarrassing) bug in our inference code, which resulted in the MRF system
producing even better results for the \emph{oof} evaluation. For the \emph{best}
evaluation, our two more sophisticated systems posted better results than the
\emph{L1} version. All of our systems beat the ``most-frequent sense" baseline
in every case.

In the following sections, we will describe our three systems, our training
data extraction process, the results on the shared task, and conclusions and
future work.

\section{L1}
The ``layer one" classifier, \emph{L1}, is a maximum entropy classifier that
uses only monolingual features from English. Although this shared task is
described as unsupervised, the L1 classifiers are trained with supervised
learning on instances that we extract programmatically from the provided
training corpus; we describe the preprocessing and training data extraction in
Section \ref{extraction}.

Having extracted the relevant training sentences from the aligned bitext for
each of the five language pairs, we created training instances with local
context features commonly used in WSD systems. These are described in Figure
\ref{fig:features}. Each instance is assigned the lemma of the translation that
was extracted from the training sentence as its label.

We trained one L1 classifier for each target language and each word of
interest, resulting in $20*5 = 100$ classifiers. We did this with the MEGA
Model optimization package \cite{daume04cg-bfgs}
\footnote{\url{http://www.umiacs.umd.edu/~hal/megam/}} and its corresponding
NLTK interface \cite{nltkbook}. Upon training, we cache these classifiers with
Python pickles, both to speed up L1 experiments and also because they are used
as components of the other models.

\begin{figure}
  \begin{itemize}  %big list
  
  \item head word features (the token to be translated)
  \begin{itemize}  %(a)
       \item literal word form (including capitalization)
       \item POS tag
       \item lemma
  \end{itemize}
  \item window unigram features (within 3 words)
  \begin{itemize} %(b)
  		\item word form
  		\item POS tag
  		\item word with POS tag
  		\item word lemma
  \end{itemize}
  \item window bigram features(within 5 words)
  \begin{itemize} %(c)
  		\item bigrams 
  		\item bigrams with POS tags
  \end{itemize}  
  \end{itemize}   %big list
  %label for the figure
  \caption{Features used in our classifiers}
  \label{fig:features}
\end{figure}


We combined the word tokens with their tags in some features so that the
classifier would not treat them independently, since maximum entropy
classifiers learn a single weight for each feature.
Particularly, the ``POS tag" feature is distinct from the ``word with tag"
feature; for the tagged word ``house/NN", the ``POS tag" feature would be $NN$, and
the ``word with tag" feature is $house\_NN$. 


\section{L2}
%% fix this up...
As in the work of Lefever and Hoste, the ``L2" model and MRF model both use
translation for four other languages as features, but in different ways.  ``L2"
model is a maxent classifier with features 1) from English source sentence (the
same features as L1), 2) translations for the target word into other four
languages.  At training time, these translations are extracted from the aligned
sentences in Europarl Intersection data.  At testing time, the translations are
estimated using the L1 classifiers. 

%Our "L2" model builds on the L1 model, but with the translations of the word of
%interest into the four other target languages as additional features.
The translations seems to be an important factor in the performance of L2,
Lefever and Hoste used a MT system for the translations instead.
???

%Unfortunately, it is difficult to evaluate which one gives better performance.
%It would be a fairer comparison of the WSD systems if a standardized way %of
%translations So as in the work of Lefever and Hoste, our L2 classifiers make
%use of several aligned bitext corpora, but without relying on a complete MT
%system.

\section{MRF}
\floatstyle{plain}
\restylefloat{figure}
\begin{figure}
  \begin{center}
  \includegraphics[width=5cm]{pentagram.pdf}
  \end{center}
  \caption{The network structure used in the MRF system: a complete five-graph
  where each node represents a variable for the translation into a target
language}
  \label{fig:pentagram}
\end{figure}

Our \emph{MRF} model builds a Markov network (often called a ``Markov random
field") of \emph{L1} classifiers in an effort to find the best translation into
all five target languages jointly. This network has nodes that correspond to
the distributions produced by the \emph{L1} classifiers, given an input
sentence, and edges with pairwise potentials that are derived from the joint
probabilities of target-language labels occurring together in the training
data. We optimize assignments over the five output variables using loopy belief
propagation \cite{DBLP:conf/uai/MurphyWJ99}.

The intuition behind using a Markov network for this task is that, since we
must make five decisions for each source-language sentence, we should make use
of the correlations between the target-langauge words. Lexical ambiguities
present translating from English to, for example, Spanish, are often similar to
the ambiguities that we see in translating to Italian. Either through cognates
or simple correlations, we expect to see regularities in the translation
process. So by building a Markov network in which all of the classifiers can
communicate (see Figure \ref{fig:pentagram}), we allow nodes to influence the
translation decisions of their neighbors, but only proportionally to the
correlation between the translations that we observe in the two languages.

%% XXX: is there some measure of the correlation between discrete nominal
%% varibles better than joint probability?

We structure our joint inference as an optimization process, in which we
minimize the sum of two kinds of penalty functions. First, from each of the
five \emph{L1} classifiers, we get a unary potential function, which assigns a
penalty to each possible label for the given word to translate, for the
particular target language of that node. The penalty assigned here is the
negative log-probability of each possible output label; the classifier returns
a probability distribution, and we map the probability values into negative-log
space.

The unary potential $\phi_i$, for some fixed set of features $f$ and a
particular language $i$, is a function from a label $l$ to some positive
penalty value.

$$
\phi_i(l) = - log P(l | f)
$$

Secondly, for each unordered pair of classifiers $(i,j)$ (\emph{i.e.}, each
edge in the graph) there is a pairwise potential function $\phi_{(i,j)}$ that
assigns a penalty to any assignment of that pair of variables.

$$
\phi_i(l_i, l_j) = - log P(L_i = l_i, L_j = l_j)
$$

Here by $P(L_i = l_i, L_j = l_j)$, we mean the probability that, for a fixed
ambiguous input word, language $i$ takes the label $l_i$ and language $j$ takes
the label $l_j$. These joint probabilities are estimated from the training
data; we count the number of times each $l_i$ and $l_j$ co-occur in the
extracted training instances and divide, with smoothing to avoid zero
probabilities and thus infinite penalties.

When it comes time to choose translations, we want to find a complete
assignment to the five variables that minimizes the sum of all of the penalties
assigned by the $\phi$ functions. As mentioned earlier, we do this via loopy
belief propagation; we use the formulation described in section XXX of
\cite{Koller+Friedman:09} where we can pass messages between the Markov network
nodes directly rather than first constructing a cluster graph.

As we are trying to compute the minimum-penalty assignment to the five
variables, we are using the \emph{min-sum} version of loopy belief propagation,
passing messages from one node to another that are mappings from the possible
values that the recipient node could take to penalty values.

At each time step, every node passes a message to each of its neighbors of the
following form:

$$
mathgoeshere
$$

Intuitively, these messages inform a given neighbor about the estimate, from
the perspective of the sending node and what it has heard from its other
neighbors, of the minimum penalty that would be incurred if the recipient node
were to take a given label. Each outgoing message depends on the sending node's
unary potential function and on the incoming messages from all neighbors other
than the recipient, at the previous time step. As a concrete example, when the
\emph{nl} node sends a message to the \emph{fr} node at time step 10, this
message is a table mapping from all known possible French translations of
the current ambiguous noun. This message depends on three things: the function
$\phi_{nl}$ (itself dependent on the probability distribution output by the
classifier), the binary potential function $\phi_{(nl,fr)}$, and the messages
from \emph{es}, \emph{it} and \emph{de} from time step 9. 

Loopy belief propagation is an approximate inference algorithm, and it is
neither guaranteed to find a globally optimal solution, nor even to converge at
all, but it does often find good solutions in practice. We run it for twenty
iterations, which empirically works well. After the message-passing iterations,
each node chooses the value that minimizes the sum of the penalties from
messages and from its own unary potential function.

\section{Training Data Extraction}
\label{extraction}
For simplicity and comparability with previous work, we worked with the
Europarl Intersection corpus provided by the task organizers. Europarl
\cite{europarl} is a parallel corpus of proceedings of the European Parliament,
available in 11 European languages, although not every sentence is translated
into every language. The Europarl Intersection is the intersection of the
sentences from Europarl that are available in English and all five of the
target languages for the task.

In order to produce the training data for the classifiers, we first tokenized
the text for all six languages with the default NLTK tokenizer and tagged the
English text with the Stanford Tagger
\cite{Toutanova03feature-richpart-of-speech}. We aligned the untagged English
with each of the target languages using the Berkeley Aligner
\cite{denero-klein:2007:ACLMain} to get one-to-many alignments from English to
target-language words, since the target-language labels may be multi-word
phrases. We used nearly the default settings for Berkeley Aligner, except that
we ran 20 iterations each of IBM Model 1 and HMM alignment.

We used TreeTagger \cite{Schmid95improvementsin} to lemmatize the text. At
first this caused some confusion in our pipeline, as TreeTagger by default
re-tokenizes input text and tries to recognize multi-word expressions. Both of
these, while sensible behaviors, were unexpected, and resulted in a surprising 
number of tokens in the TreeTagger output. Once we turned off these behaviors,
TreeTagger provided useful lemmas for all of the languages.

%% I'd just say that obviously this would not be possible
%% in real-world disambiguation.
Given the tokenized and aligned sentences, with their part-of-speech tags and
lemmas, we used a number of heuristics to extract the appropriate
target-language labels for each English-language input sentence.  For each
ambiguous noun in the shared task, we extracted a sense inventory $V_i$ from
the gold standard answers from the 2010 iteration of this task
\cite{lefever-hoste:2009:SEW}. Then, for each sentence that contains one of the
ambiguous nouns (XXX what's the best term for this?  ``target noun" ?), we
examine the alignments to determine whether that noun is aligned with a sense
present in $V_i$ , or whether the words aligned to that noun are a subsequence
of such a sense. The same check is performed both on the lemmatized and
unlemmatized versions of the target-language sentence. If so, then that sense
from the gold standard $V_i$ is taken to be the label for this sentence. While
a gold standard sense inventory will clearly not be present for general
translation systems, there will be some vocabulary of possible translations for
each word, taken from a bilingual dictionary or the phrase table in a
phrase-based SMT system.

If a label from $V_i$ is not found with the alignments, but some other word or
phrase is aligned with the ambiguous noun, then we believe the output of the
aligner, and the lemmatized version of this target-language phrase is taken to
be the label for this sentence.

We dropped all of the training instances with labels that only occurred once,
considering them likely alignment errors or other noise.

%% XXX: do we need to talk about this?
%% The quality of training data depends on the alignments and lemmatization, in
%% Els work XXX, manual aligning and lemmatizing was involved to provide an upper
%% bound.

\section{Results}
\begin{table}[t!]
  \begin{center}
    \begin{tabular}{|r|r|r|r|r|r|}
      \hline
      system   & es    & nl    & de    &  it   & fr \\
      \hline
   MFS  & 23.23          & 20.66          & 17.43          & 20.21          & 25.74 \\
   best & 32.16          & 23.61          & 20.82          & 25.66          & 30.11 \\
      \hline
            L1 & 29.01          & 21.53          & 19.5           & 24.52          & 27.01 \\
            L2 & 28.49          & \textbf{22.36} & \textbf{19.92} & 23.94          & \textbf{28.23} \\
           MRF & \textbf{29.36} & 21.61          & 19.76          & \textbf{24.62} & 27.46 \\
      \hline
    \end{tabular}
  \caption{\emph{best} evaluation results: precision}
  \label{table:resultsbest}
  \end{center}
\end{table}

\begin{table}[t!]
  \begin{center}
    \begin{tabular}{|r|r|r|r|r|r|}
      \hline
      system   & es    & nl    & de    &  it   & fr \\
      \hline
    MFS & 53.07          & 43.59              & 38.86          & 42.63          & 51.36 \\
   best & 62.21          & 47.83              & 44.02          & 53.98          & 59.8 \\
      \hline
           L1  & 61.69          & 46.55              & 43.66          & 53.57          & 57.76 \\
           L2  & 59.51          & 46.36              & 42.32          & 53.05          & \textbf{58.2} \\
           MRF & \textbf{62.21} & \textbf{46.63}     & \textbf{44.02} & \textbf{53.98} & 57.83 \\
      \hline
    \end{tabular}
  \caption{\emph{oof} evaluation results: precision}
  \label{table:resultsbest}
  \end{center}
\end{table}

%% edit and expand!
For the \emph{best} evaluation, the more sophisticated classifiers usually do
better, though not always. It's not totally clear that they're better in
general.

It seems like Els's features really are richer -- she gets translations for all
the different words in the source language and uses the other-target-language
bag-of-words as features. That's a lot of features. We're kind of forcing the
information through a narrower pass -- we just get one decision.

TODO(alexr): now we know what went wrong. Write about it!

TODO(alexr): run the experiment where we do the L2 classifiers, but with the
\emph{predicted} target-language word as the feature.

\section{Conclusions and future work}
Our systems had a strong showing in the competition, always beating the MFS
baseline and achieving the top score for three of the five languages in the
\emph{oof} evaluation. The systems that took into account evidence from
multiple sources had better performance than the simpler one: our top result in
every language came from either the \emph{L2} or the \emph{MRF} classifier for
both evaluations. This suggests that it is possible to make use of the evidence
in several parallel corpora in a CL-WSD task without translating every word in
a source sentence into many target languages. However, for the \emph{best}
evaluation, our system (XXX check!!) did not perform as well as some other
systems, such as those of (XXX cite) Marine and Marten, or as well as ParaSense
... (check the ParaSense results)

%% IMPORTANT QUESTION: does our
%% L2 work as well as Els on the 2010 trial data? Should run that whole
%% experiment.

We expect that the \emph{L2} classifier could be improved by adding features
derived
from more classifiers. Particularly, we hypothesize that classifiers trained on
completely different corpora -- instead of corpora that are mutally parallel --
would improve the system by making use of information from many disparate
sources. The \emph{L2} classifier approach only requires that the first-layer
classifiers make \emph{some} prediction based on text in the source language.
They need not be trained from the same source text, depend on the same
features, or even output words as labels. In future work we will explore
all of these variations. One could, for example, train a monolingual WSD system
on a sense-tagged corpus \footnote{Such as the sense-tagged corpus from Wordnet
(\url{http://wordnet.princeton.edu/glosstag.shtml})
or the new ``WikiLinks" corpus from Google and UMass Amherst, in which text is
tagged with ``sense labels" that are links to disambiguated Wikipedia
articles
(\url{http://googleresearch.blogspot.com/2013/03/learning-from-big-data-40-million.html})}
and use this as an additional information source for an \emph{L2} classifier.

There remain a number avenues that we would like to explore for the MRF system;
thus far, we have used the joint probability of two labels to set the binary
potentials. We would like to investigate other functions, especially ones that
do not incur large penalties for rare labels, as the joint probability of two
labels that often co-occur but are both rare will be low.  Also, in the current
system, the relative weights of the binary potentials and the unary potentials
were set by hand, with a very small amount of empirical tuning. We could, in
the future, tune the weights with a more principled optimization strategy,
using a development set.

%%problem with joint probability as potentials, conditional probability as
%%potentials, we could abuse the mathematics of MRF a little bit and experiment
%%on this approximate potential.

As with the \emph{L2} classifiers, it would be helpful in the future for the
MRF system to not require many mutually parallel corpora for training --
however, the current approach for estimating the edge potentials requires the
use of bitext for each edge in the network, or at least source-language text
with labels for both target languages in question. Perhaps these correlations
could be estimated in a semi-supervised way, with high-confidence automatic
labels being used to estimate the joint distribution over target-language
phrases.  We would also like to investigate approaches to jointly disambiguate
many words in the same sentence, since lexical ambiguity is not just a problem
for a few nouns.

Aside from improvements to the design of our CL-WSD system itself, we want to
use it in a practical system for translating into under-resourced languages.
We are now working on integrating this project with our rule-based MT system,
$L^3$ \cite{gasser:aflat2012}. We had experimented with a similar, though less
sophisticated, CL-WSD system for Quechua \cite{rudnick:2011:RANLPStud}, but in
the future $L^3$, with the integrated CL-WSD system, should be capable of
translating Spanish to Guarani, either as a standalone system, or as part of a
computer-assisted translation tool.

%% \section*{Acknowledgments}
%% Do not number the acknowledgment section.

\bibliographystyle{naaclhlt2013.bst}
\bibliography{semeval2013.bib}{}

\end{document}
