\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2013}
\usepackage{times}
\usepackage{latexsym}
\setlength\titlebox{6.5cm}    % Expanding the titlebox
\usepackage{url}
\usepackage{float}
\usepackage{graphicx}
\floatstyle{boxed}
\restylefloat{figure}

%% already told Els that this was the title, can't change it now.
\title{HLTDI: CL-WSD Using Markov Random Fields for SemEval-2013 Task 10}

\author{Alex Rudnick, Can Liu and Michael Gasser\\
	    Indiana University, School of Informatics and Computing \\
	    {\tt \{alexr,liucan,gasser\}@indiana.edu}}

\date{}

\begin{document}
\maketitle

%what resource did we use, 
\begin{abstract}

We present our entries for the SemEval-2013 cross-language word-sense
disambiguation task \cite{task10}. We submitted three systems based on
classifiers trained on local context features, with some elaborations. Our
three systems, in increasing order of complexity, were: maximum entropy
classifiers trained to predict the desired target-language phrase using only
monolingual features (we called this system \emph{L1}); similar classifiers,
but with the desired target-language phrase for the other four languages as
features (\emph{L2}); and lastly, networks of five classifiers, over which we
do loopy belief propagation to solve the classification tasks jointly
(\emph{MRF}).
\end{abstract}

\section{Introduction}
In the cross-language word-sense disambiguation (CL-WSD) task, given an
instance of an ambiguous word used in a context, we want to predict the
appropriate translation into some target language. This setting for WSD has an
immediate application in machine translation, since many words have many
possible translations. Framing the resolution of lexical ambiguities as an
explicit classification task has been shown to be improve machine translation
even in the case of phrase-based SMT systems \cite{carpuatpsd}, which can
mitigate lexical ambiguities through the use of a language model and
phrase-tables with multi-word phrases.

XXX: work in Brown 1991 reference too: 
\cite{Brown91word-sensedisambiguation}

In the Semeval-2013 CL-WSD task \cite{task10}, entrants are asked to build a
system that can provide translations for twenty ambiguous English nouns, given
appropriate contexts. The five target languages in the shared task are Spanish,
Dutch, German, Italian and French. There were two settings for the evaluation,
``best" and ``oof". In either case, systems may present multiple possible
answers for a given translation, although in the ``best" setting, the first
answer is given more weight in the evaluation, and this setting encourages only
returning the top answer. In the ``oof" setting, systems are asked to
return the top-five most likely translations. For a complete explanation of the
task and its evaluation, please see the shared task description \cite{task10}.

%% consider: maybe move this to related work?
Following the work of Lefever and Hoste
\shortcite{lefever-hoste-decock:2011:ACL-HLT2011}, we wanted to develop systems
that make use of multiple bitext corpora for the CL-WSD task.  ParaSense, the
system of Lefever and Hoste, takes into account evidence from all of the
available parallel corpora. Let $S$ be the set of five target languages and $t$
be the particular target language of interest at the moment; ParaSense creates
bag-of-words features from the translations of the target sentence into the
languages $S - \lbrace{t \rbrace}$. Given corpora that are parallel over many
languages, this is straightforward to do at training time, however at testing
time it requires the use of a complete MT system into the four other languages,
which is computationally prohibitive. Thus in our work, we have developed
systems that make use of many parallel corpora but require neither a locally
running MT system nor access to an online translation API.

We presented three systems in this competition, which were variations on the
theme of a maximum entropy classifier for each ambiguous noun, trained on local
context features similar to those used in previous work and familiar from the
WSD literature.

Our systems had similar results, but at the time of the evaluation, our
simplest system came in first place for the out-of-five evaluation for three
languages (Spanish, German, and Italian).  However, after the evaluation
deadline, we fixed a simple (slightly embarrassing) bug in our MRF code, which
resulted in the MRF system producing even better results for the OOF
evaluation.

... on the \emph{oof} evaluation, we had the best results for Spanish, German,
and Italian.  All of our systems beat the ``most-frequent sense" baseline in
every case.

Our three systems made use of the same training data, which we extracted from
the Europarl Intersection corpus, meaning that the English-language source
sentences were identical for each of the five corpora. But the source-language
sentences need not be identical, and we plan to explore relaxing this
restriction in future work.

In the following sections, we will describe our three systems, followed by the
common preprocessing steps that we performed, and then discuss results and
future work.

We will describe our systems in a increasing order of complexity.
\section{L1}
preprocessing, training, testing\\

The one-layer classifier (which we call L1), is a Maximum Entropy (Multinomial Logistic Regression) classifier that uses only monolingual features from English.
Although this shared task is performed in an unsupervised manner (there is no annotated corpora), L1 was constructed in a supervised way.
With bitext of English and target language available from EuroParl, we extracted training instances from aligned sentences, where we used rich local context features(described below).
The L1 was trained for each target language and for each word of interest. 

An important part of building L1 is extracting relevant and good training instances. 
(We feel that it would be fairer if a standard way is provided..but maybe not it's unsupervised anyway XXX ???)
We used alignments from Berkeley Aligner to locate the corresponding translation for a source English word,
and lemmatize this translation using TreeTagger to produce a label.  
The quality of training data relies much on the alignments and lemmatization, in Els work XXX, manual aligning and lemmatizing was involved to provide an upper bound.
In it an  


Once we find translation pairs that we want to include in the training data
(mappings from English-language sentences and instances of the ambiguous word
in question to the appropriate target-language lemma), we extract features from
the English-language sentence. 

%% rework a bit
Several steps of preprocessing were needed. We first POS tagged the sentences,
since we are only interested in nouns.  Then align the words in each sentence
pair, and lemmatize the target sentence.  After locating words of interest in
the Europarl Intersection corpus, training instances were extracted, and a
maxent classifier was trained over local context features similar to those used
by Lefever and Hoste.

%% howto do a nested list?
\begin{figure}
  \begin{itemize}
  \item word form
  \item word tag (word with tag?)
  \item word lemma
  \item for words in the three-word window on both sides
  \item wordform, lemma, pos tag
  \item for words in a five-word window on both sides
  \item bigrams and tagged bigrams (just in case)
  \end{itemize}
  \label{features}
  \caption{Features used in our classifiers}
\end{figure}

Note: word tag is different from word with tag (so as for bigram and bigram
with tag). eg. ``NN" is a tag for ``bank", but ``bank/NN" is the word with tag.
We include this because tags are good indicator of word senses, and the senses
of context helps disambiguate the target word. 

megam for learning \footnote{\url{http://www.umiacs.umd.edu/~hal/megam/}}
\cite{daume04cg-bfgs}
\section{L2}
As in the work of Lefever and Hoste, the ``L2" model and MRF model both use
translation for four other languages as features, but in different ways.  ``L2"
model is a maxent classifier with features 1) from English source sentence (the
same features as L1), 2) translation for the target word into other four
languages.  At training time, these translations are extracted from the aligned
sentences in Europarl Intersection data.  At testing time, the translations are
estimated using the L1 classifiers. 

%Our "L2" model builds on the L1 model, but with the translations of the word of
%interest into the four other target languages as additional features.
The translations seems to be an important factor in the performance of L2,
Lefever and Hoste used a MT system for the translations instead.
???

%Unfortunately, it is difficult to evaluate which one gives better performance.
%It would be a fairer comparison of the WSD systems if a standardized way %of
%translations So as in the work of Lefever and Hoste, our L2 classifiers make
%use of several aligned bitext corpora, but without relying on a complete MT
%system.

\section{MRF}
\floatstyle{plain}
\restylefloat{figure}
\begin{figure}
  \begin{center}
  \includegraphics[width=5cm]{pentagram.pdf}
  \end{center}
  \label{pentagram}
  \caption{The network structure used in the MRF system: a complete five-graph
  where each node represents a variable for the translation into a target
language}
\end{figure}
Our "MRF" model builds a Markov network of L1 classifiers in an effort to find
the best translation into all five target languages jointly. This network has
nodes that correspond to the distribution produced by the L1 classifiers, and
edges with pairwise potentials that are derived from the joint probabilities of
target-language labels occurring together in the training data. We optimize
assignments over the five output variables using loopy belief propagation

The idea of MRF is that translations for five target languages must agree with
each other, since these languages are all related.  It is then natural to ask:
how related are they and can MRF represent different closeness between
languages?  ???  We know that Dutch is more related to German than with
Spanish, Italian and French. Can this closeness be represented by the pairwise
potentials?


There was some concern about pairwise potential in MRF, which is joint probability. Consider a word which occurs 500 times in the training data, it could co-occur with
We had some concern about pairwise potential in MRF, which is joint
probability. Consider a word which occurs 500 times in the training data, it
could co-occur with

The future work should experiment on different weight we put on the pairwise
potentials...

problem with joint probability as potentials, conditional probability as
potentials, we could abuse the mathematics of MRF a little bit and experiment
on this approximate potential.

\section{Results}
\begin{table*}[t!]
  \begin{center}
    \begin{tabular}{|r|r|r|r|r|r|}
      \hline
      system   & es    & nl    & de    &  it   & fr \\
      \hline
      baseline & 23.23          & 20.66          & 17.43          & 20.21          & 25.74 \\
   best result & 32.16          & 23.61          & 20.82          & 25.66          & 30.11 \\
      \hline
            L1 & 29.01          & 21.53          & 19.5           & 24.52          & 27.01 \\
            L2 & 28.49          & \textbf{22.36} & \textbf{19.92} & 23.94          & \textbf{28.23} \\
           MRF & \textbf{29.36} & 21.61          & 19.76          & \textbf{24.62} & 27.46 \\
      \hline
    \end{tabular}
  \caption{``best" evaluation results: precision}
  \label{table:resultsbest}
  \end{center}
\end{table*}

\begin{table*}[t!]
  \begin{center}
    \begin{tabular}{|r|r|r|r|r|r|}
      \hline
      system   & es    & nl    & de    &  it   & fr \\
      \hline
      baseline & 53.07          & 43.59              & 38.86          & 42.63          & 51.36 \\
   best result & 61.69          & 47.83              & 44.02          & 53.98          & 59.8 \\
      \hline
           L1  & 61.69          & 46.55              & 43.66          & 53.57          & 57.76 \\
           L2  & 59.51          & 46.36              & 42.32          & 53.05          & \textbf{58.2} \\
           MRF & \textbf{62.21} & \textbf{46.63}     & \textbf{44.02} & \textbf{53.98} & 57.83 \\
      \hline
    \end{tabular}
  \caption{``oof" evaluation results: precision}
  \label{table:resultsbest}
  \end{center}
\end{table*}

%% edit and expand!
For the \emph{best} evaluation, the more sophisticated classifiers usually do
better, though not always. It's not totally clear that they're better in
general.

It seems like Els's features really are richer -- she gets translations for all
the different words in the source language and uses the other-target-language
bag-of-words as features. That's a lot of features. We're kind of forcing the
information through a narrower pass -- we just get one decision.

TODO(alexr): now we know what went wrong. Write about it!

TODO(alexr): run the experiment where we do the L2 classifiers, but with the
\emph{predicted} target-language word as the feature.


\section{Training Data Extraction}
For simplicity and comparability with previous work, we worked entirely with
the Europarl Intersection corpus provided by the task organizers.  Europarl
\cite{europarl} is a parallel corpus of proceedings of the European
Parliament, which is available in 11 languages, although not every sentence is
translated into every language. The Europarl Intersection is the intersection
of the sentences from Europarl that are available in English and all five of
the target languages for the task.

In order to produce the training data for the classifiers, we first tokenized
the text for all six languages with the default NLTK tokenizer \cite{nltkbook},
and tagged the English text with the Stanford Tagger
\cite{Toutanova03feature-richpart-of-speech}. We aligned the untagged English
with each of the target languages using the Berkeley Aligner
\cite{denero-klein:2007:ACLMain}, to get one-to-many alignments from English to
target-language words, since the target-language labels may be multi-word
phrases. We used nearly the default settings for Berkeley Aligner, except
that we ran 20 iterations each of IBM Model 1 and HMM alignment.

We used TreeTagger \cite{Schmid95improvementsin} to lemmatize the text. At
first this caused some confusion in our pipeline, as TreeTagger by default
re-tokenizes input text and tries to recognize multi-word expressions. Both of
these, while sensible behaviors, were unexpected, and resulted in a surprising 
number of tokens in the TreeTagger output.

After all of the preprocessing,  ...

We also cheated a bit by using the gold standard labels from last time, looking
for instances of the gold labels in the target text; if our noun had an
alignment into those gold standard labels, we preferred taking that gold label
(labels can be multi-word expressions).

We dropped all of the training instances with labels that only occurred once,
considering them likely alignment errors or other noise.

\section{Conclusions and future work}
Our systems had a strong showing in the competition, always beating the MFS
baseline and achieving the top score for three of the five languages in the OOF
evaluation. Our systems that took into account evidence from multiple sources
had better performance than the simpler one: our top result came from either
the L2 or the MRF classifier for both evaluations, for each language.

We expect that the L2 classifier could be improved by adding features derived
from more classifiers. Particularly, we hypothesize that classifiers trained on
completely different corpora -- instead of corpora that are mutally parallel --
would improve the system by making use of information from many disparate
sources. The L2 classifier approach only requires that its the first-layer
classifiers make \emph{some} prediction based on text in the source language.
They need not be trained from the same source text, depend on the same
features, or even output words as labels. In future work we will explore
all of these variations. One could, for example, train a monolingual WSD system
on a sense-tagged corpus \footnote{Perhaps the sense-tagged corpus from Wordnet
(\url{http://wordnet.princeton.edu/glosstag.shtml})
or the new ``WikiLinks" corpus from Google and UMass Amherst, in which text is
tagged with ``sense labels" that are just links to disambiguated Wikipedia
articles
(\url{http://googleresearch.blogspot.com/2013/03/learning-from-big-data-40-million.html})}
and use this as an extra information source for an L2 classifier.

%% an argument against this is that if you have so many bitext corpora, why
%% don't you just train up a whole SMT system for each language pair? Then you
%% could do Els's approach of getting bag-of-words features for your
%% classification. But that's a pain, and do you really want to run Joshua five
%% times every time you want to translate a noun? IMPORTANT QUESTION: does our
%% L2 work as well as Els on the 2010 trial data? Should run that whole
%% experiment.


For the MRF classifiers ...
come up with better ways to set the edge weights for the MRF. We tried doing
the negative logprob of joint probabilities (and then weighting those with
sort of an arbitrary hyperparameter), and that intuitively seems like it might
be the right thing, but we don't know. We should have tested more. What would
be really great would be a way to estimate these probabilities even if we
can't get them from training data

As we are primarily interested in MT for low-resourced and disadvantaged
languages, we plan to adapt this system to Spanish/Guarani MT in the near
future -- earlier experiments were done with a similar, though less
sophisticated, system for Quechua \cite{rudnick:2011:RANLPStud}.

%% \section*{Acknowledgments}
%% Do not number the acknowledgment section.

\bibliographystyle{naaclhlt2013.bst}
\bibliography{semeval2013.bib}{}

\end{document}
