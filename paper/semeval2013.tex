\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2013}
\usepackage{times}
\usepackage{latexsym}
\setlength\titlebox{6.5cm}    % Expanding the titlebox
\usepackage{url}
\usepackage{float}
\usepackage{graphicx}
\floatstyle{boxed}
\restylefloat{figure}

%% already told Els that this was the title, can't change it now.
\title{HLTDI: CL-WSD Using Markov Random Fields for SemEval-2013 Task 10}

\author{Alex Rudnick, Can Liu and Michael Gasser\\
	    Indiana University, School of Informatics and Computing \\
	    {\tt \{alexr,liucan,gasser\}@indiana.edu}}

\date{}

\begin{document}
\maketitle

\begin{abstract}
We present our entries for the SemEval-2013 cross-language word-sense
disambiguation task \cite{task10}. We submitted three systems based on
classifiers trained on local context features, with some elaborations. Our
three systems, in increasing order of complexity, were: maximum entropy
classifiers trained to predict the desired target-language phrase using only
monolingual features (we called this system \emph{L1}); similar classifiers,
but with the desired target-language phrase for the other four languages as
features (\emph{L2}); and lastly, networks of five classifiers, over which we
do loopy belief propagation to solve the classification tasks jointly
(\emph{MRF}).
\end{abstract}

\section{Introduction}
In the cross-language word-sense disambiguation (CL-WSD) task, given an
instance of an ambiguous word used in a context, we want to predict the
appropriate translation into some target language. This setting for WSD has an
immediate application in machine translation, since many words have many
possible translations. Framing the resolution of lexical ambiguities as an
explicit classification task has been shown to be improve machine translation
even in the case of phrase-based SMT systems \cite{carpuatpsd}, which can
mitigate lexical ambiguities through the use of a language model and
phrase-tables with multi-word phrases.

XXX: work in Brown 1991 reference too: 
\cite{Brown91word-sensedisambiguation}

In the Semeval-2013 CL-WSD task \cite{task10}, entrants are asked to build a
system that can provide translations for twenty ambiguous English nouns, given
appropriate contexts. The five target languages in the shared task are Spanish,
Dutch, German, Italian and French. There were two settings for the evaluation,
\emph{best} and \emph{oof}. In either case, systems may present multiple
possible answers for a given translation, although in the \emph{best} setting,
the first answer is given more weight in the evaluation, and this setting
encourages only returning the top answer. In the \emph{oof} setting, systems
are asked to return the top-five most likely translations. For a complete
explanation of the task and its evaluation, please see the shared task
description \cite{task10}.

%% consider: maybe move this to related work?
Following the work of Lefever and Hoste
\shortcite{lefever-hoste-decock:2011:ACL-HLT2011}, we wanted to develop systems
that make use of multiple bitext corpora for the CL-WSD task.  ParaSense, the
system of Lefever and Hoste, takes into account evidence from all of the
available parallel corpora. Let $S$ be the set of five target languages and $t$
be the particular target language of interest at the moment; ParaSense creates
bag-of-words features from the translations of the target sentence into the
languages $S - \lbrace{t \rbrace}$. Given corpora that are parallel over many
languages, this is straightforward to do at training time, however at testing
time it requires the use of a complete MT system into the four other languages,
which is computationally prohibitive. Thus in our work, we have developed
systems that make use of many parallel corpora but require neither a locally
running MT system nor access to an online translation API.

We presented three systems in this competition, which were variations on the
theme of a maximum entropy classifier for each ambiguous noun, trained on local
context features similar to those used in previous work and familiar from the
WSD literature.

Our systems had similar results, but at the time of the evaluation, our
simplest system came in first place for the out-of-five evaluation for three
languages (Spanish, German, and Italian).  However, after the evaluation
deadline, we fixed a simple (slightly embarrassing) bug in our MRF code, which
resulted in the MRF system producing even better results for the OOF
evaluation.

... on the \emph{oof} evaluation, we had the best results for Spanish, German,
and Italian.  All of our systems beat the ``most-frequent sense" baseline in
every case.

Our three systems made use of the same training data, which we extracted from
the Europarl Intersection corpus, meaning that the English-language source
sentences were identical for each of the five corpora. But the source-language
sentences need not be identical, and we plan to explore relaxing this
restriction in future work.

In the following sections, we will describe our three systems in increasing
order of sophistication, the results on the shared task, common preprocessing
steps, and conclusions and future work.

\section{L1}
The one-layer classifier (which we call L1), is a maximum entropy classifier
that uses only monolingual features from English.  Although this shared task is
described as unsupervised, the L1 classifiers are trained with supervised
learning; we describe the preprocessing and training data extraction in Section
\ref{extraction}.

Having extracted the relevant training sentences from the aligned bitext for
each of the five language pairs, we created training instances with local
context features commonly used in WSD systems. These are described in Figure 1
(XXX fix this \ref{features}). Each instance is assigned the lemma of the
translation that was extracted from the training sentence as its
label.

We trained one L1 classifier for each target language and each word of
interest, resulting in $20*5 = 100$ classifiers. We did this with the MEGA
Model optimization package \cite{daume04cg-bfgs}
\footnote{\url{http://www.umiacs.umd.edu/~hal/megam/}} and its corresponding
NLTK interface \cite{nltkbook}. Upon training, we cache these classifiers with
Python pickles, both to speed up L1 experiments and also because they are used
as components of the other models.

\begin{figure}
  \begin{itemize}  %big list
  
  \item head word features (the token to be translated)
  \begin{itemize}  %(a)
       \item literal word form (including capitalization)
       \item POS tag
       \item lemma
  \end{itemize}
  \item Window Unigram Features (within 3 words)
  \begin{itemize} %(b)
  		\item word form
  		\item POS tag
  		\item word with tag
  		\item word lemma
  \end{itemize}
  \item Window Bigram Features(within 5 words)
  \begin{itemize} %(c)
  		\item bigrams 
  		\item bigrams with tags
  \end{itemize}  
  
  
  \end{itemize}   %big list
  %label for the figure
  \label{features}
  \caption{Features used in our classifiers}
\end{figure}


We combined the word tokens with their tags in some features so that the
classifier would not treat them independently, since maximum entropy
classifiers learn a single weight for each feature.
Particularly, the ``POS tag" feature is distinct from the ``word with tag"
feature; for the tagged word ``house/NN", the ``POS tag" feature would be $NN$, and
the ``word with tag" feature is $house\_NN$. 


\section{L2}
As in the work of Lefever and Hoste, the ``L2" model and MRF model both use
translation for four other languages as features, but in different ways.  ``L2"
model is a maxent classifier with features 1) from English source sentence (the
same features as L1), 2) translation for the target word into other four
languages.  At training time, these translations are extracted from the aligned
sentences in Europarl Intersection data.  At testing time, the translations are
estimated using the L1 classifiers. 

%Our "L2" model builds on the L1 model, but with the translations of the word of
%interest into the four other target languages as additional features.
The translations seems to be an important factor in the performance of L2,
Lefever and Hoste used a MT system for the translations instead.
???

%Unfortunately, it is difficult to evaluate which one gives better performance.
%It would be a fairer comparison of the WSD systems if a standardized way %of
%translations So as in the work of Lefever and Hoste, our L2 classifiers make
%use of several aligned bitext corpora, but without relying on a complete MT
%system.

\section{MRF}
\floatstyle{plain}
\restylefloat{figure}
\begin{figure}
  \begin{center}
  \includegraphics[width=5cm]{pentagram.pdf}
  \end{center}
  \label{pentagram}
  \caption{The network structure used in the MRF system: a complete five-graph
  where each node represents a variable for the translation into a target
language}
\end{figure}
Our "MRF" model builds a Markov network of L1 classifiers in an effort to find
the best translation into all five target languages jointly. This network has
nodes that correspond to the distribution produced by the L1 classifiers, and
edges with pairwise potentials that are derived from the joint probabilities of
target-language labels occurring together in the training data. We optimize
assignments over the five output variables using loopy belief propagation

The idea of MRF is that translations for five target languages must agree with
each other, since these languages are all related.  It is then natural to ask:
how related are they and can MRF represent different closeness between
languages?  ???  We know that Dutch is more related to German than with
Spanish, Italian and French. Can this closeness be represented by the pairwise
potentials?


There was some concern about pairwise potential in MRF, which is joint probability. Consider a word which occurs 500 times in the training data, it could co-occur with
We had some concern about pairwise potential in MRF, which is joint
probability. Consider a word which occurs 500 times in the training data, it
could co-occur with

The future work should experiment on different weight we put on the pairwise
potentials...

problem with joint probability as potentials, conditional probability as
potentials, we could abuse the mathematics of MRF a little bit and experiment
on this approximate potential.


\section{Training Data Extraction}
\label{extraction}

XXX: write something about how this is common to all the other steps

For simplicity and comparability with previous work, we worked entirely with
the Europarl Intersection corpus provided by the task organizers.  Europarl
\cite{europarl} is a parallel corpus of proceedings of the European
Parliament, which is available in 11 languages, although not every sentence is
translated into every language. The Europarl Intersection is the intersection
of the sentences from Europarl that are available in English and all five of
the target languages for the task.

In order to produce the training data for the classifiers, we first tokenized
the text for all six languages with the default NLTK tokenizer,
and tagged the English text with the Stanford Tagger
\cite{Toutanova03feature-richpart-of-speech}. We aligned the untagged English
with each of the target languages using the Berkeley Aligner
\cite{denero-klein:2007:ACLMain}, to get one-to-many alignments from English to
target-language words, since the target-language labels may be multi-word
phrases. We used nearly the default settings for Berkeley Aligner, except
that we ran 20 iterations each of IBM Model 1 and HMM alignment.

We used TreeTagger \cite{Schmid95improvementsin} to lemmatize the text. At
first this caused some confusion in our pipeline, as TreeTagger by default
re-tokenizes input text and tries to recognize multi-word expressions. Both of
these, while sensible behaviors, were unexpected, and resulted in a surprising 
number of tokens in the TreeTagger output.

After all of the preprocessing,  ...

We also cheated a bit by using the gold standard labels from last time, looking
for instances of the gold labels in the target text; if our noun had an
alignment into those gold standard labels, we preferred taking that gold label
(labels can be multi-word expressions).

We dropped all of the training instances with labels that only occurred once,
considering them likely alignment errors or other noise.

%% move into extraction -- maybe this bit could be in future work, like
%% suggestions ...
(We feel that it would be fairer if a standard way is provided..but maybe not
it's unsupervised anyway XXX ???) We first locate of source word of interest by
POS tagging the English corpora, then we used alignments from Berkeley Aligner
to locate the corresponding translation for a source English word, and
lemmatize this translation using TreeTagger to produce a label.

Labels that appear once in the training data were dropped, because they are likely to be errors and noise.
The quality of training data relies much on the alignments and lemmatization, in Els work XXX, manual aligning and lemmatizing was involved to provide an upper bound.
%If we were to fully make use of available parallel corpora, ensuring quality
%alignments and lemmatization is    ...



\section{Results}
\begin{table}[t!]
  \begin{center}
    \begin{tabular}{|r|r|r|r|r|r|}
      \hline
      system   & es    & nl    & de    &  it   & fr \\
      \hline
   MFS  & 23.23          & 20.66          & 17.43          & 20.21          & 25.74 \\
   best & 32.16          & 23.61          & 20.82          & 25.66          & 30.11 \\
      \hline
            L1 & 29.01          & 21.53          & 19.5           & 24.52          & 27.01 \\
            L2 & 28.49          & \textbf{22.36} & \textbf{19.92} & 23.94          & \textbf{28.23} \\
           MRF & \textbf{29.36} & 21.61          & 19.76          & \textbf{24.62} & 27.46 \\
      \hline
    \end{tabular}
  \caption{\emph{best} evaluation results: precision}
  \label{table:resultsbest}
  \end{center}
\end{table}

\begin{table}[t!]
  \begin{center}
    \begin{tabular}{|r|r|r|r|r|r|}
      \hline
      system   & es    & nl    & de    &  it   & fr \\
      \hline
    MFS & 53.07          & 43.59              & 38.86          & 42.63          & 51.36 \\
   best & 62.21          & 47.83              & 44.02          & 53.98          & 59.8 \\
      \hline
           L1  & 61.69          & 46.55              & 43.66          & 53.57          & 57.76 \\
           L2  & 59.51          & 46.36              & 42.32          & 53.05          & \textbf{58.2} \\
           MRF & \textbf{62.21} & \textbf{46.63}     & \textbf{44.02} & \textbf{53.98} & 57.83 \\
      \hline
    \end{tabular}
  \caption{\emph{oof} evaluation results: precision}
  \label{table:resultsbest}
  \end{center}
\end{table}

%% edit and expand!
For the \emph{best} evaluation, the more sophisticated classifiers usually do
better, though not always. It's not totally clear that they're better in
general.

It seems like Els's features really are richer -- she gets translations for all
the different words in the source language and uses the other-target-language
bag-of-words as features. That's a lot of features. We're kind of forcing the
information through a narrower pass -- we just get one decision.

TODO(alexr): now we know what went wrong. Write about it!

TODO(alexr): run the experiment where we do the L2 classifiers, but with the
\emph{predicted} target-language word as the feature.

\section{Conclusions and future work}
Our systems had a strong showing in the competition, always beating the MFS
baseline and achieving the top score for three of the five languages in the OOF
evaluation. The systems that took into account evidence from multiple sources
had better performance than the simpler one: our top result in every language
came from either the L2 or the MRF classifier for both evaluations. This
suggests that it is possible to make use of the evidence in several parallel
corpora in a CL-WSD task without translating every word in a source sentence
into many target languages. However, for the \emph{best} evaluation, our system
(XXX check!!) did not perform as well as some other systems, such as those of
(XXX cite) Marine and Marten, or as well as ParaSense ... (check the ParaSense
results)

%% IMPORTANT QUESTION: does our
%% L2 work as well as Els on the 2010 trial data? Should run that whole
%% experiment.

We expect that the L2 classifier could be improved by adding features derived
from more classifiers. Particularly, we hypothesize that classifiers trained on
completely different corpora -- instead of corpora that are mutally parallel --
would improve the system by making use of information from many disparate
sources. The L2 classifier approach only requires that its the first-layer
classifiers make \emph{some} prediction based on text in the source language.
They need not be trained from the same source text, depend on the same
features, or even output words as labels. In future work we will explore
all of these variations. One could, for example, train a monolingual WSD system
on a sense-tagged corpus \footnote{Perhaps the sense-tagged corpus from Wordnet
(\url{http://wordnet.princeton.edu/glosstag.shtml})
or the new ``WikiLinks" corpus from Google and UMass Amherst, in which text is
tagged with ``sense labels" that are links to disambiguated Wikipedia
articles
(\url{http://googleresearch.blogspot.com/2013/03/learning-from-big-data-40-million.html})}
and use this as an extra information source for an L2 classifier.

There remain a number avenues that we would like to explore for the MRF system;
thus far, we have used the joint probability of two labels to set the binary
potentials. We would like to investigate other functions, especially possibly
ones that do not incur large penalties for rare labels, as the joint
probability of two labels that often co-occur but are both rare will be low.
Also, in the current system, the relative weights of the binary potentials and
the unary potentials were set by hand, with a very small amount of empirical
tuning. We could, in the future, tune the weights with a more principled
optimization strategy, using a development set.

As with the L2 classifiers, it would be helpful in the future for the MRF
system to not require many mutually parallel corpora for training -- however,
the current approach for estimating the edge potentials requires the use of
bitext for each edge in the network, or at least source-language text with
labels for both target languages in question. Perhaps these correlations could
be estimated in a semi-supervised way, with high-confidence automatic labels
being used to estimate the joint distribution over target-language phrases.  We
would also like to investigate approaches to jointly disambiguate many words in
the same sentence, since lexical ambiguity is not just a problem for a few
nouns.

Aside from improvements to the design of our CL-WSD system itself, we want to
use it in a practical system for translating into under-resourced languages.
We are now working on integrating this project with our rule-based MT system,
$L^3$ \cite{gasser:aflat2012}. We had experimented with a similar, though less
sophisticated, CL-WSD system for Quechua \cite{rudnick:2011:RANLPStud}, but in
the future $L^3$, with the integrated CL-WSD system, should be capable of
translating Spanish to Guarani, either as a standalone system, or as part of a
computer-assisted translation tool.

%% \section*{Acknowledgments}
%% Do not number the acknowledgment section.

\bibliographystyle{naaclhlt2013.bst}
\bibliography{semeval2013.bib}{}

\end{document}
