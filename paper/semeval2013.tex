\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2013}
\usepackage{times}
\usepackage{latexsym}
\setlength\titlebox{6.5cm}    % Expanding the titlebox
\usepackage{url}
\usepackage{float}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{breqn}
\usepackage{caption}
\usepackage{subcaption}
\captionsetup{compatibility=false}
\floatstyle{boxed}
\restylefloat{figure}

%% already told Els that this was the title, can't change it now.
\title{HLTDI: CL-WSD Using Markov Random Fields for SemEval-2013 Task 10}

\author{Alex Rudnick, Can Liu and Michael Gasser\\
	    Indiana University, School of Informatics and Computing \\
	    {\tt \{alexr,liucan,gasser\}@indiana.edu}}

\date{}

\begin{document}
\maketitle

\begin{abstract}
We present our entries for the SemEval-2013 cross-language word-sense
disambiguation task \cite{task10}. We submitted three systems based on
classifiers trained on local context features, with some elaborations. Our
three systems, in increasing order of complexity, were: maximum entropy
classifiers trained to predict the desired target-language phrase using only
monolingual features (we called this system L1); similar classifiers,
but with the desired target-language phrase for the other four languages as
features (L2); and lastly, networks of five classifiers, over which we
do loopy belief propagation to solve the classification tasks jointly
(MRF).
\end{abstract}

%%% TODO items
% TODO: run the experiment where we train the L2 classifiers on the predicted
% values.

% DONE: pick consistent terminology for the word that we're trying to translate
% The word in the task description is the ``target" word. I guess we should use
% that.

\section{Introduction}
In the cross-language word-sense disambiguation (CL-WSD) task, given an
instance of an ambiguous word used in a context, we want to predict the
appropriate translation into some target language. This setting for WSD has an
immediate application in machine translation, since many words have multiple
possible translations. Framing the resolution of lexical ambiguities as an
explicit classification task has a long history, and was considered in early
SMT work at IBM \cite{Brown91word-sensedisambiguation}. More recently, Carpuat
and Wu have shown how to use CL-WSD techniques to improve modern phrase-based
SMT systems \cite{carpuatpsd}, even though the language model and phrase-tables
of these systems mitigate the problem of lexical ambiguities somewhat.

In the Semeval-2013 CL-WSD task \cite{task10}, entrants are asked to build a
system that can provide translations for twenty ambiguous English nouns, given
appropriate contexts -- here the particular usage of the ambiguous noun is
called the \emph{target} word. The five target languages of the shared task are
Spanish, Dutch, German, Italian and French. There were two settings for the
evaluation, \emph{best} and \emph{oof}. In either case, systems may present
multiple possible answers for a given translation, although in the \emph{best}
setting, the first answer is given more weight in the evaluation, and the
scoring encourages only returning the top answer. In the \emph{oof} setting,
systems are asked to return the top-five most likely translations. For a
complete explanation of the task and its evaluation, please see the shared task
description. 

Following the work of Lefever and Hoste
\shortcite{lefever-hoste-decock:2011:ACL-HLT2011}, we wanted to make use of
multiple bitext corpora for the CL-WSD task. ParaSense, the system of Lefever
and Hoste, takes into account evidence from all of the available parallel
corpora. Let $S$ be the set of five target languages and $t$ be the particular
target language of interest at the moment; ParaSense creates bag-of-words
features from the translations of the target sentence into the languages $S -
\lbrace{t \rbrace}$. Given corpora that are parallel over many languages, this
is straightforward at training time. However at testing time it requires a
complete MT system for each of the four other languages, which is
computationally prohibitive. Thus in our work, we learn from several parallel
corpora but require neither a locally running MT system nor access to an online
translation API.

We presented three systems in this shared task, all of which were variations on
the theme of a maximum entropy classifier for each ambiguous noun, trained on
local context features similar to those used in previous work and familiar from
the WSD literature. The first system, L1 (``layer one"), uses maximum entropy
classifiers trained on local context features. The second system, L2 (``layer
two"), is the same as the L1 system, with the addition of the correct
translations into the other target languages as features, which at testing time
are predicted with L1 classifiers. The third system, MRF (``Markov random
field") uses a network of interacting classifiers to solve the classification
problem for all five target languages jointly. Our three systems are all
trained from the same data, which we extracted from the Europarl Intersection
corpus provided by the shared task organizers.

At the time of the evaluation, our simplest system had the top results in the
shared task for the out-of-five evaluation for three languages (Spanish,
German, and Italian).  However, after the evaluation deadline, we fixed a
simple bug in our MRF code, and the MRF system then achieved even better
results for the \emph{oof} evaluation. For the \emph{best} evaluation, our two
more sophisticated systems posted better results than the L1 version. All of
our systems beat the ``most-frequent sense" baseline in every case.

In the following sections, we will describe our three
systems\footnote{\url{http://github.iu.edu/alexr/semeval2013}}, our training
data extraction process, the results on the shared task, and conclusions and
future work.

\section{L1}
The ``layer one" classifier, L1, is a maximum entropy classifier that
uses only monolingual features from English. Although this shared task is
described as unsupervised, the L1 classifiers are trained with supervised
learning on instances that we extract programmatically from the provided
training corpus; we describe the preprocessing and training data extraction in
Section \ref{extraction}.

Having extracted the relevant training sentences from the aligned bitext for
each of the five language pairs, we created training instances with local
context features commonly used in WSD systems. These are described in Figure
\ref{fig:features}. Each instance is assigned the lemma of the translation that
was extracted from the training sentence as its label.

We trained one L1 classifier for each target language and each word of
interest, resulting in $20*5 = 100$ classifiers. Classifiers were trained with
the MEGA Model optimization package \cite{daume04cg-bfgs}
\footnote{\url{http://www.umiacs.umd.edu/~hal/megam/}} and its corresponding
NLTK interface \cite{nltkbook}. Upon training, we cache these classifiers with
Python pickles, both to speed up L1 experiments and also because they are used
as components of the other models.

\begin{figure}
  \begin{itemize}  %big list
  
  \item target word features
  \begin{itemize}  %(a)
       \item literal word form (including capitalization)
       \item POS tag
       \item lemma
  \end{itemize}
  \item window unigram features (within 3 words)
  \begin{itemize} %(b)
  		\item word form
  		\item POS tag
  		\item word with POS tag
  		\item word lemma
  \end{itemize}
  \item window bigram features(within 5 words)
  \begin{itemize} %(c)
  		\item bigrams 
  		\item bigrams with POS tags
  \end{itemize}  
  \end{itemize}   %big list
  %label for the figure
  \caption{Features used in our classifiers}
  \label{fig:features}
\end{figure}


We combined the word tokens with their tags in some features so that the
classifier would not treat them independently, since maximum entropy
classifiers learn a single weight for each feature.
Particularly, the ``POS tag" feature is distinct from the ``word with tag"
feature; for the tagged word ``house/NN", the ``POS tag" feature would be $NN$, and
the ``word with tag" feature is $house\_NN$. 

\section{L2}
The ``layer two" classifier, L2, is similar to L1 described above, with the
addition of multilingual features, particularly the translation of the target
word the four other target languages. More specifically, let $S$ be the set of
five target languages, and $w$ be the target word that we want to translate
into $t$. Then for the L2 classifier, we use translations of $w$ into the other
four languages in $S - \left\lbrace t \right\rbrace$ as features. At training
time, these translations are extracted from EuroParl Intersection data, since
we have available the translations of each of the English sentences into each
of the five target languages; the appropriate translations are extracted from
the parallel sentences as described in section \ref{extraction}. At testing
time, since the parallel corpora are not available, translations for $w$ in the
four other languages are estimated using the cached L1 classifiers.

Levefer and Hoste \shortcite{lefever-hoste-decock:2011:ACL-HLT2011} used the
Google Translate API to translate the source English sentences into the four
other languages in $S - \left\lbrace t \right\rbrace$, and extracted
bag-of-words features from these complete sentences. The L2 classifiers make
use of a similar intuition, but they do not rely on a complete MT system or an
available online MT API; we only include the translations of the target words
as features.

%Unfortunately, it is difficult to evaluate which one gives better performance.
%It would be a fairer comparison of the WSD systems if a standardized way %of
%translations So as in the work of Lefever and Hoste, our L2 classifiers make
%use of several aligned bitext corpora, but without relying on a complete MT
%system.

\section{MRF}
\floatstyle{plain}
\restylefloat{figure}
\begin{figure}
  \begin{center}
  \includegraphics[width=5cm]{pentagram.pdf}
  \end{center}
  \caption{The network structure used in the MRF system: a complete graph with five nodes %five-graph
  where each node represents a variable for the translation into a target
language}
  \label{fig:pentagram}
\end{figure}

Our MRF model builds a Markov network (often called a ``Markov random
field") of L1 classifiers in an effort to find the best translation into
all five target languages jointly. This network has nodes that correspond to
the distributions produced by the L1 classifiers, given an input
sentence, and edges with pairwise potentials that are derived from the joint
probabilities of target-language labels occurring together in the training
data. 
Thus the task of finding the optimal translations into five languages jointly
is framed as a MAP (Maximum A Posteriori) inference problem, where we try to
maximize the joint probability $P(w_{fr},w_{es},w_{it},w_{de},w_{nl})$, given
the evidence of the features extracted from the source-language sentence. The
inference process is performed using loopy belief propagation
\cite{DBLP:conf/uai/MurphyWJ99}, which is an approximate but tractable
inference algorithms that, while it gives no guarantees, often produces good
solutions in practice.

The intuition behind using a Markov network for this task is that, since we
must make five decisions for each source-language sentence, we should make use
of the correlations between the target-langauge words. Correlations might occur
in practice due to cognates -- the languages in the shared task are fairly
closely related -- or they simply capture ambiguities in the source language
that are not common.

So by building a Markov network in which all of the classifiers can
communicate (see Figure \ref{fig:pentagram}), we allow nodes to influence the
translation decisions of their neighbors, but only proportionally to the
correlation between the translations that we observe in the two languages.

%% keep writing and editing
The joint inference on MRF could be structured as a MAP inference process (as described above),
which is a maximization problem by itself, but a simple negative log transformation could turn this into
a minimization problem, in which we minimize the sum of two distinct penalty functions.

First, we have a unary function from each of the five L1 classifiers and for
each target language, which is represented as a node in the group.  The
function assigns a penalty to each possible label for the target word.  The
penalty assigned here is the negative log-probability of each possible output
label; the classifier returns a probability distribution, and we map the
probability values $[0,1]$ into negative-log space, $[0, +\infty]$.

The unary potential $\phi_i$, for some fixed set of features $f$ and a
particular language $i$, is a function from a label $l$ to some positive
penalty value.

$$
\phi_i(l) = - log P(l | f)
$$

Secondly, for each unordered pair of classifiers $(i,j)$ (\emph{i.e.}, each
edge in the graph) there is a pairwise potential function $\phi_{ij}$ that
assigns a penalty to any assignment of that pair of variables.

$$
\phi_{(i,j)}(l_i, l_j) = - log P(L_i = l_i, L_j = l_j)
$$

Here by $P(L_i = l_i, L_j = l_j)$, we mean the probability that, for a fixed
ambiguous input word, language $i$ takes the label $l_i$ and language $j$ takes
the label $l_j$. These joint probabilities are estimated from the training
data; we count the number of times each pair of labels $l_i$ and $l_j$
co-occurs in the training sentences and divide, with smoothing to avoid zero
probabilities and thus infinite penalties.

When it comes time to choose translations, we want to find a complete
assignment to the five variables that minimizes the sum of all of the penalties
assigned by the $\phi$ functions. As mentioned earlier, we do this via loopy
belief propagation; we use the formulation described in section 11.3.5.1 of
\cite{Koller+Friedman:09}, passing messages between the Markov network nodes
directly rather than first constructing a cluster graph.

As we are trying to compute the minimum-penalty assignment to the five
variables, we use the \emph{min-sum} version of loopy belief propagation,
passing messages that are mappings from the possible values that the recipient
node could take to penalty values.

At each time step, every node passes to each of its neighbors a message of the
following form:

\begin{dmath*}
\delta_{i \rightarrow j}^{t} (L_j) =
  \min_{l_i \in L_i}
  \phi_i(l_i) +
  \phi_{(i,j)}(l_i, l_j) +
  \sum_{k \in S - \lbrace i,j \rbrace}
  \delta_{k \rightarrow i}^{t-1} (l_i)
\end{dmath*}

By this expression, we mean that the message from node $i$ to node $j$ at time
$t$ is a function from possible labels for node $j$ to scalar penalty values.
Each penalty value is determined by minimizing over the possible labels for
node $i$, such that we find the label $l_i$ that minimizes sum of the unary
cost for that label, the binary cost for $l_i$ and $l_j$ taken jointly, and all
of the penalties in the messages that node $i$ received at the previous time
step, except for the one from node $j$.

Intuitively, these messages inform a given neighbor about the estimate, from
the perspective of the sending node and what it has heard from its other
neighbors, of the minimum penalty that would be incurred if the recipient node
were to take a given label. As a concrete example, when the \emph{nl} node
sends a message to the \emph{fr} node at time step 10, this message is a table
mapping from all possible French translations of the current target word
to their associated penalty values. The message depends on three things: the
function $\phi_{nl}$ (itself dependent on the probability distribution output
by the classifier), the binary potential function $\phi_{(nl,fr)}$, and the
messages from \emph{es}, \emph{it} and \emph{de} from time step 9.  Note that
the binary potential functions are symmetric because they are derived from
joint probabilities.

Loopy belief propagation is an approximate inference algorithm, and it is
neither guaranteed to find a globally optimal solution, nor even to converge at
all, but it does often find good solutions in practice. We run it for twenty
iterations, which empirically works well. After the message-passing iterations,
each node chooses the value that minimizes the sum of the penalties from
messages and from its own unary potential function. To avoid accumulating very
large penalties, we normalize the outgoing messages at each time step and give
a larger weight to the unary potential functions. These normalization and
weighting parameters were set by hand, but seem to work well in practice.

\section{Training Data Extraction}
\label{extraction}
For simplicity and comparability with previous work, we worked with the
Europarl Intersection corpus provided by the task organizers. Europarl
\cite{europarl} is a parallel corpus of proceedings of the European Parliament,
available in 11 European languages, although not every sentence is translated
into every language. The Europarl Intersection is the intersection of the
sentences from Europarl that are available in English and all five of the
target languages for the task.

In order to produce the training data for the classifiers, we first tokenized
the text for all six languages with the default NLTK tokenizer and tagged the
English text with the Stanford Tagger
\cite{Toutanova03feature-richpart-of-speech}. We aligned the untagged English
with each of the target languages using the Berkeley Aligner
\cite{denero-klein:2007:ACLMain} to get one-to-many alignments from English to
target-language words, since the target-language labels may be multi-word
phrases. We used nearly the default settings for Berkeley Aligner, except that
we ran 20 iterations each of IBM Model 1 and HMM alignment.

We used TreeTagger \cite{Schmid95improvementsin} to lemmatize the text. At
first this caused some confusion in our pipeline, as TreeTagger by default
re-tokenizes input text and tries to recognize multi-word expressions. Both of
these, while sensible behaviors, were unexpected, and resulted in a surprising 
number of tokens in the TreeTagger output. Once we turned off these behaviors,
TreeTagger provided useful lemmas for all of the languages.

%% I'd just say that obviously this would not be possible
%% in real-world disambiguation.
Given the tokenized and aligned sentences, with their part-of-speech tags and
lemmas, we used a number of heuristics to extract the appropriate
target-language labels for each English-language input sentence.  For each
target word, we extracted a sense inventory $V_i$ from the gold standard
answers from the 2010 iteration of this task \cite{lefever-hoste:2009:SEW}.
Then, for each sentence that contains one of the target words used as a noun,
we examine the alignments to determine whether that word is aligned with a
sense present in $V_i$ , or whether the words aligned to that noun are a
subsequence of such a sense. The same check is performed both on the lemmatized
and unlemmatized versions of the target-language sentence. If we do find a
match, then that sense from the gold standard $V_i$ is taken to be the label
for this sentence. While a gold standard sense inventory will clearly not be
present for general translation systems, there will be some vocabulary of
possible translations for each word, taken from a bilingual dictionary or the
phrase table in a phrase-based SMT system.

If a label from $V_i$ is not found with the alignments, but some other word or
phrase is aligned with the ambiguous noun, then we trust the output of the
aligner, and the lemmatized version of this target-language phrase is assigned
as the label for this sentence. In this case we used some heuristic functions
to remove stray punctuation and attached articles (such as \emph{d'} from
French or \emph{nell'} from Italian) that were often left appended to the
tokens by the default NLTK English tokenizer.

We dropped all of the training instances with labels that only occurred once,
considering them likely alignment errors or other noise.

\section{Results}
\begin{figure*}[th!]
\begin{subfigure}[t]{0.55\textwidth}
  \begin{tabular}{|r|r|r|r|r|r|}
    \hline
    system   & es    & nl    & de    &  it   & fr \\
    \hline
 MFS  & 23.23          & 20.66          & 17.43          & 20.21          & 25.74 \\
 best & 32.16          & 23.61          & 20.82          & 25.66          & 30.11 \\
   PS & 31.72 & 25.29 & 24.54 & 28.15 & 31.21 \\
    \hline
          L1 & 29.01          & 21.53          & 19.5           & 24.52          & 27.01 \\
          L2 & 28.49          & \textbf{22.36} & \textbf{19.92} & 23.94          & \textbf{28.23} \\
         MRF & \textbf{29.36} & 21.61          & 19.76          & \textbf{24.62} & 27.46 \\
    \hline
  \end{tabular}
\caption{\emph{best} evaluation results: precision}
\label{table:resultsbest}
\end{subfigure}
%
\begin{subfigure}[t]{0.4\textwidth}
    \begin{tabular}{|r|r|r|r|r|r|}
      \hline
      system   & es    & nl    & de    &  it   & fr \\
      \hline
    MFS & 53.07          & 43.59              & 38.86          & 42.63          & 51.36 \\
   best & 62.21          & 47.83              & 44.02          & 53.98          & 59.80 \\
      \hline
           L1  & 61.69          & 46.55              & 43.66          & 53.57          & 57.76 \\
           L2  & 59.51          & 46.36              & 42.32          & 53.05          & \textbf{58.20} \\
           MRF & \textit{\textbf{62.21}}& \textbf{46.63} & \textit{\textbf{44.02}}& \textit{\textbf{53.98}} & 57.83 \\
      \hline
    \end{tabular}
\caption{\emph{oof} evaluation results: precision}
\label{table:resultsoof}
\end{subfigure}

\bigskip

\begin{subfigure}[t]{0.55\textwidth}
    \begin{tabular}{|r|r|r|r|r|r|}
      \hline
system & es    & nl    & de    &  it   & fr \\
      \hline
MFS    & 27.48 & 24.15 & 15.30 & 19.88 & 20.19 \\
best   & 37.11 & 27.96 & 24.74 & 31.61 & 26.62 \\
PS     & 40.26 & 30.29 & 25.48 & 30.11 & 26.33 \\
      \hline
L1  & 36.32          & 25.39          & 24.16          & 26.52          & \textbf{21.24} \\
L2  & \textit{\textbf{37.11}} & 25.34 & \textit{\textbf{24.74}} & \textbf{26.65} & 21.07 \\
MRF & 36.57          & \textbf{25.72} & 24.01          & 26.26          & \textbf{21.24} \\
      \hline
    \end{tabular}
\caption{\emph{best} evaluation results: mode precision}
\label{table:resultsmodebest}
\end{subfigure}
%
\begin{subfigure}[t]{0.4\textwidth}
    \begin{tabular}{|r|r|r|r|r|r|}
      \hline
system  & es    & nl    & de    &  it   & fr \\
      \hline
MFS & 57.35 & 41.97 & 44.35 & 41.69 & 47.42 \\
best& 65.10 & 47.34 & 53.75 & 57.50 & 57.57 \\
      \hline
L1  & 64.65                   & \textit{\textbf{47.34}} & 53.50                   & 56.61                   & 51.96 \\
L2  & 62.52                   & 44.06                   & 49.03                   & 54.06                   & \textbf{53.57} \\
MRF & \textit{\textbf{65.10}} & 47.29                   & \textit{\textbf{53.75}} & \textit{\textbf{57.50}} & 52.14 \\
      \hline
    \end{tabular}
\caption{\emph{oof} evaluation results: mode precision}
\label{table:resultsoof}
\end{subfigure}

\caption{Task results for our systems. Scores in \textbf{bold} are the best
result for that language and evaluation out of our systems, and those in
\textbf{\textit{bold italics}} are the best posted in the competition. For
comparison, we also give scores for the most-frequent-sense baseline (``MFS"),
ParaSense (``PS"), the system developed by Lefever and Hoste, and the best
posted score for competing systems this year (``best").}
\label{fig:theresults}
\end{figure*}

There were two evaluations in the shared task, \emph{best} and \emph{oof}. In
the \emph{best} evaluation ... and in the \emph{oof} evaluation ...
Both evaluations also include a ``mode" variant, in which only the most popular
of the gold-standard answers are considered valid.

The scores for our systems are reported in \ref{fig:theresults}. In all of the
settings, our systems posted some of the top results among entrants in the
shared task, achieving the best scores for some evaluations and some languages.
For every setting and language, our systems beat the most-frequent sense
baseline, and our best results usually came from either the L2 or MRF system,
which suggests that they are making use of the information from the multiple
parallel corpora. For the \emph{best} evaluation, our results were lower than
those posted by ParaSense, and in the non-mode \emph{best} setting, they were
also lower than those from the \emph{c1lN} system \cite{maarten} and
\emph{adapt1} \cite{marine}.

Thus there are many improvements that could be made to our system; perhaps we
could integrate ideas from the other entries in the shared task this year.
The ParaSense approach, including the entire the entire bag-of-words for
translated sentences other languages ...

%% XXX keep writing and editing here.
... features, making use of a translated features really are richer -- she gets
translations for all the different words in the source language and uses the
other-target-language bag-of-words as features. That's a lot of features. We're
kind of forcing the information through a narrower pass -- we just get one
decision.

For the \emph{best} evaluation, considering only the mode gold-standard
answers, our L2 system achieved the highest scores in the competition for
Spanish and German. For the \emph{oof} evaluation, our MRF system -- with its
post-competition bug fix -- posted the best results for Spanish, German and
Italian in both complete and mode variants. Curiously, our L1 system posted the
best results in the competition for Dutch in the \emph{oof} variant 



\section{Conclusions and future work}
Our systems had a strong showing in the competition, always beating the MFS
baseline and achieving the top score for three of the five languages in the
\emph{oof} evaluation. The systems that took into account evidence from
multiple sources had better performance than the simpler one: our top result in
every language came from either the L2 or the MRF classifier for
both evaluations. This suggests that it is possible to make use of the evidence
in several parallel corpora in a CL-WSD task without translating every word in
a source sentence into many target languages. However, for the \emph{best}
evaluation, our system (XXX check!!) did not perform as well as some other
systems, such as those of (XXX cite) Marine and Marten, or as well as ParaSense
... (check the ParaSense results)

We expect that the L2 classifier could be improved by adding features
derived
from more classifiers. Particularly, we hypothesize that classifiers trained on
completely different corpora -- instead of corpora that are mutally parallel --
would improve the system by making use of information from many disparate
sources. The L2 classifier approach only requires that the first-layer
classifiers make \emph{some} prediction based on text in the source language.
They need not be trained from the same source text, depend on the same
features, or even output words as labels. In future work we will explore
all of these variations. One could, for example, train a monolingual WSD system
on a sense-tagged corpus \footnote{Such as the sense-tagged corpus from Wordnet
(\url{http://wordnet.princeton.edu/glosstag.shtml})
or the new ``WikiLinks" corpus from Google and UMass Amherst, in which text is
tagged with ``sense labels" that are links to disambiguated Wikipedia
articles
(\url{http://googleresearch.blogspot.com/2013/03/learning-from-big-data-40-million.html})}
and use this as an additional information source for an L2 classifier.

There remain a number of avenues that we would like to explore for the MRF
system; thus far, we have used the joint probability of two labels to set the
binary potentials. We would like to investigate other functions, especially
ones that do not incur large penalties for rare labels, as the joint
probability of two labels that often co-occur but are both rare will be low.
Also, in the current system, the relative weights of the binary potentials and
the unary potentials were set by hand, with a very small amount of empirical
tuning. We could, in the future, tune the weights with a more principled
optimization strategy, using a development set.

As with the L2 classifiers, it would be helpful in the future for the
MRF system to not require many mutually parallel corpora for training --
however, the current approach for estimating the edge potentials requires the
use of bitext for each edge in the network, or at least source-language text
with labels for both target languages in question. Perhaps these correlations
could be estimated in a semi-supervised way, with high-confidence automatic
labels being used to estimate the joint distribution over target-language
phrases.  We would also like to investigate approaches to jointly disambiguate
many words in the same sentence, since lexical ambiguity is not just a problem
for a few nouns.

Aside from improvements to the design of our CL-WSD system itself, we want to
use it in a practical system for translating into under-resourced languages.
We are now working on integrating this project with our rule-based MT system,
$L^3$ \cite{gasser:aflat2012}. We had experimented with a similar, though less
sophisticated, CL-WSD system for Quechua \cite{rudnick:2011:RANLPStud}, but in
the future $L^3$, with the integrated CL-WSD system, should be capable of
translating Spanish to Guarani, either as a standalone system, or as part of a
computer-assisted translation tool.

%% \section*{Acknowledgments}
%% Do not number the acknowledgment section.

\bibliographystyle{naaclhlt2013.bst}
\bibliography{semeval2013.bib}{}

\end{document}
